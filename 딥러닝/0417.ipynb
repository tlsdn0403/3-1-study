{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4ed7d8-7a4d-4163-9c3c-f52429cd1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#미분치\n",
    "def sigmoid_deriv(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "learning_rate = 0.2\n",
    "\n",
    "input = [[0,0],[0,1],[1,0],[1,1]]\n",
    "\n",
    "targets = [1, 0, 0, 1]\n",
    "\n",
    "#은닉층 1\n",
    "w1_11 = 0.10\n",
    "w1_21 = 0.30\n",
    "b1_1 = 0.1\n",
    "# 은닉층 2\n",
    "w1_12 = 0.10\n",
    "w1_22 = 0.30\n",
    "b1_2 = 0.1\n",
    "\n",
    "# 은닉층 2\n",
    "w1_13 = 0.10\n",
    "w1_23 = 0.30\n",
    "b1_3 = 0.1\n",
    "\n",
    "#출력층\n",
    "w2_11 = 0.50\n",
    "w2_21 = 0.60\n",
    "w2_31 = 0.30\n",
    "b2 = 0.3\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    global w1_11 ,w1_21 ,b1_1 , w1_12 , w1_22, b1_2, w1_13 , w1_23, b1_3, w2_11 ,w2_21 ,w2_31 ,b2\n",
    "\n",
    "    x1 , x2 =x\n",
    "    z1 = x1 * w1_11+ x2 * w1_21 + b1_1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    z2 = x1 * w1_12+ x2 * w1_22 + b1_2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    z3 = x1 * w1_13+ x2 * w1_23 + b1_3\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    z_out = a1 * w2_11 + a2 * w2_21+ a3 * w2_31 + b2\n",
    "    a_out =sigmoid(z_out)\n",
    "\n",
    "    return a1, a2 , a3, a_out\n",
    "\n",
    "def backward(x, target ,a1, a2 , a3, a_out):\n",
    "    global w1_11 ,w1_21 ,b1_1 , w1_12 , w1_22, b1_2, w1_13 , w1_23, b1_3, w2_11 ,w2_21 ,w2_31 ,b2\n",
    "\n",
    "    x1, x2 = x\n",
    "    error = a_out - target\n",
    "    delta_out = error * sigmoid_deriv(a_out)\n",
    "\n",
    "    delta1 = delta_out * w2_11 * sigmoid_deriv(a1)\n",
    "    delta2 = delta_out * w2_21 * sigmoid_deriv(a2)\n",
    "    delta3 = delta_out * w2_31 * sigmoid_deriv(a3)\n",
    "\n",
    "    w2_11 -= learning_rate * a1 * delta_out\n",
    "    w2_21 -= learning_rate * a2 * delta_out\n",
    "    w2_31 -= learning_rate * a3 * delta_out\n",
    "    b2 -= learning_rate * delta_out\n",
    "\n",
    "    w1_11 -= learning_rate * x1 * delta1\n",
    "    w1_21 -= learning_rate * x2 * delta1\n",
    "    b1_1 -= learning_rate *delta1\n",
    "    \n",
    "    w1_12 -= learning_rate * x1 * delta1\n",
    "    w1_22 -= learning_rate * x2 * delta1\n",
    "    b1_2 -= learning_rate *delta1\n",
    "\n",
    "    w1_13 -= learning_rate * x1 * delta1\n",
    "    w1_23 -= learning_rate * x2 * delta1\n",
    "    b1_3 -= learning_rate *delta1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37768bb6-7a1d-4aeb-b6b8-a892bf5db219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#은닉층과 출력층의 손실함수 적용 결과에 따른 가중치 수정을 설명\n",
    "\n",
    "#시그모이드 함수\n",
    "\n",
    "def actf(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#미분치\n",
    "def actf_deriv(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "inputs, hiddens, outputs = 2, 2, 1\n",
    "\n",
    "leraning_rate = 0.2\n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "T = np.array([[1],[0],[0],[1]])\n",
    "\n",
    "#가중치\n",
    "W1 = np.array([[0.1, 0.2],[0.3, 0.4]])\n",
    "B1 = np.array([[0.1, 0.2]])\n",
    "W2 = np.array([[0.5],[0.6]])\n",
    "B2 = np.array([0.3])\n",
    "w\n",
    "def predict(x):\n",
    "    layer0 = x\n",
    "    Z1 = np.dot(layer0, W1) + B1\n",
    "    layer1 = actf(Z1)\n",
    "    Z2 = np.dot(layer1, W2) + B2\n",
    "    layer2 = actf(Z2)\n",
    "    return layer0, layer1, layer2\n",
    "\n",
    "#역전파 계산\n",
    "def fit():\n",
    "    global W1, W2, B1, B2\n",
    "    for i in range(10):\n",
    "        for x, y in zip(X, T):\n",
    "            x = np.reshape(x,(1, -1))\n",
    "            y = np.reshape(y,(1, -1))\n",
    "\n",
    "            layer0, layer1, layer2 = predict(x)\n",
    "            #print(layer0, layer1, layer2)\n",
    "            layer2_error = layer2-y\n",
    "            #print(layer2_error)\n",
    "            layer2_delta = layer2_error*actf_deriv(layer2)\n",
    "            #print(layer2_delta)\n",
    "            #print(W2.T)\n",
    "            layer1_error = np.dot(layer2_delta, W2.T)\n",
    "            #print(layer1_error)\n",
    "            layer1_delta = layer1_error*actf_deriv(layer1)\n",
    "\n",
    "            W2 += -leraning_rate*np.dot(layer1.T,layer2_delta)\n",
    "            W1 += -leraning_rate*np.dot(layer0.T,layer1_delta)\n",
    "            B2 += -leraning_rate*np.sum(layer2_delta,axis=0)\n",
    "            B1 += -leraning_rate*np.sum(layer1_delta,axis=0)\n",
    "\n",
    "def test():\n",
    "    for x, y in zip(X, T):\n",
    "        x = np.reshape(x, (1, -1))\t# 하나의 샘플을 꺼내서 2차원 행렬로 만든다. \n",
    "        layer0, layer1, layer2 = predict(x)\n",
    "        print(x, y, layer2)\t\t# 출력층의 값을 출력해본다. \n",
    "\n",
    "fit()\n",
    "test()\n",
    "                                                \n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d7eab9c-6b07-498d-9397-d0fc5a6cd49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n",
      "epoch= 0 ==================\n",
      "현재 처리 입력= [0 0 1] 정답= 0 predict= 0 변형된 가중치= [0. 0. 0.]\n",
      "현재 처리 입력= [0 1 1] 정답= 1 predict= 0 변형된 가중치= [0.  0.2 0.2]\n",
      "현재 처리 입력= [1 0 1] 정답= 1 predict= 1 변형된 가중치= [0.  0.2 0.2]\n",
      "현재 처리 입력= [1 1 1] 정답= 0 predict= 1 변형된 가중치= [-0.2  0.   0. ]\n",
      "====================\n",
      "epoch= 1 ==================\n",
      "현재 처리 입력= [0 0 1] 정답= 0 predict= 0 변형된 가중치= [-0.2  0.   0. ]\n",
      "현재 처리 입력= [0 1 1] 정답= 1 predict= 0 변형된 가중치= [-0.2  0.2  0.2]\n",
      "현재 처리 입력= [1 0 1] 정답= 1 predict= 0 변형된 가중치= [0.  0.2 0.4]\n",
      "현재 처리 입력= [1 1 1] 정답= 0 predict= 1 변형된 가중치= [-0.2  0.   0.2]\n",
      "====================\n",
      "epoch= 2 ==================\n",
      "현재 처리 입력= [0 0 1] 정답= 0 predict= 1 변형된 가중치= [-0.2  0.   0. ]\n",
      "현재 처리 입력= [0 1 1] 정답= 1 predict= 0 변형된 가중치= [-0.2  0.2  0.2]\n",
      "현재 처리 입력= [1 0 1] 정답= 1 predict= 0 변형된 가중치= [0.  0.2 0.4]\n",
      "현재 처리 입력= [1 1 1] 정답= 0 predict= 1 변형된 가중치= [-0.2  0.   0.2]\n",
      "====================\n",
      "epoch= 3 ==================\n",
      "현재 처리 입력= [0 0 1] 정답= 0 predict= 1 변형된 가중치= [-0.2  0.   0. ]\n",
      "현재 처리 입력= [0 1 1] 정답= 1 predict= 0 변형된 가중치= [-0.2  0.2  0.2]\n",
      "현재 처리 입력= [1 0 1] 정답= 1 predict= 0 변형된 가중치= [0.  0.2 0.4]\n",
      "현재 처리 입력= [1 1 1] 정답= 0 predict= 1 변형된 가중치= [-0.2  0.   0.2]\n",
      "====================\n",
      "epoch= 4 ==================\n",
      "현재 처리 입력= [0 0 1] 정답= 0 predict= 1 변형된 가중치= [-0.2  0.   0. ]\n",
      "현재 처리 입력= [0 1 1] 정답= 1 predict= 0 변형된 가중치= [-0.2  0.2  0.2]\n",
      "현재 처리 입력= [1 0 1] 정답= 1 predict= 0 변형된 가중치= [0.  0.2 0.4]\n",
      "현재 처리 입력= [1 1 1] 정답= 0 predict= 1 변형된 가중치= [-0.2  0.   0.2]\n",
      "====================\n",
      "epoch= 5 ==================\n",
      "현재 처리 입력= [0 0 1] 정답= 0 predict= 1 변형된 가중치= [-0.2  0.   0. ]\n",
      "현재 처리 입력= [0 1 1] 정답= 1 predict= 0 변형된 가중치= [-0.2  0.2  0.2]\n",
      "현재 처리 입력= [1 0 1] 정답= 1 predict= 0 변형된 가중치= [0.  0.2 0.4]\n",
      "현재 처리 입력= [1 1 1] 정답= 0 predict= 1 변형된 가중치= [-0.2  0.   0.2]\n",
      "====================\n",
      "0 0 -> 1\n",
      "0 1 -> 1\n",
      "1 0 -> 0\n",
      "1 1 -> 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "epsilon = 0.000001\n",
    "\n",
    "def step_func(t):\n",
    "    if t > epsilon: \n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "X = np.array([[0, 0, 1],[0, 1, 1],[1, 0, 1],\n",
    "            [1, 1, 1]])\n",
    "\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "W = np.zeros(len(X[0]))\n",
    "print(W)\n",
    "\n",
    "def perceptron_fit(X, Y, epochs=100):\n",
    "    global W\n",
    "\n",
    "    eta = 0.2 # 학습률\n",
    "    \n",
    "    for t in range(epochs):\n",
    "\n",
    "        print(\"epoch=\", t, \"==================\")\n",
    "        for i in range(len(X)):\n",
    "\n",
    "            predict = step_func(np.dot(X[i],W)) #x랑 w 각 행 값을 곱헤서 더한다고 생각 -> 내적을 구함\n",
    "            error =  Y[i] - predict\n",
    "            # print(\"변경전 가중치 = \",W)\n",
    "            # print(\"error=\", error)\n",
    "            # print(\"X[i] = \",X[i])\n",
    "            #W += eta * error * X[i]\n",
    "            Wx = eta * error * X[i]\n",
    "            #print(\"변경할 가중치 = \", Wx)\n",
    "            W = W+Wx\n",
    "\n",
    "            print(\"현재 처리 입력=\",X[i], \"정답=\",Y[i], \"predict=\", predict,\"변형된 가중치=\",W)\n",
    "        print(\"====================\")\n",
    "def perceptron_predict(X, Y):\n",
    "    global W\n",
    "    for x in X:\n",
    "        print(x[0], x[1], \"->\", step_func(np.dot(x, W)))\n",
    "\n",
    "\n",
    "\n",
    "perceptron_fit(X, y, 6)\n",
    "perceptron_predict(X, y)\n",
    "\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c69ab024-c635-4780-9f7d-dfd5c82e912a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06169621]\n",
      " [-0.05147406]\n",
      " [ 0.04445121]\n",
      " [-0.01159501]\n",
      " [-0.03638469]\n",
      " [-0.04069594]\n",
      " [-0.04716281]\n",
      " [-0.00189471]\n",
      " [ 0.06169621]\n",
      " [ 0.03906215]\n",
      " [-0.08380842]\n",
      " [ 0.01750591]\n",
      " [-0.02884001]\n",
      " [-0.00189471]\n",
      " [-0.02560657]\n",
      " [-0.01806189]\n",
      " [ 0.04229559]\n",
      " [ 0.01211685]\n",
      " [-0.0105172 ]\n",
      " [-0.01806189]\n",
      " [-0.05686312]\n",
      " [-0.02237314]\n",
      " [-0.00405033]\n",
      " [ 0.06061839]\n",
      " [ 0.03582872]\n",
      " [-0.01267283]\n",
      " [-0.07734155]\n",
      " [ 0.05954058]\n",
      " [-0.02129532]\n",
      " [-0.00620595]\n",
      " [ 0.04445121]\n",
      " [-0.06548562]\n",
      " [ 0.12528712]\n",
      " [-0.05039625]\n",
      " [-0.06332999]\n",
      " [-0.03099563]\n",
      " [ 0.02289497]\n",
      " [ 0.01103904]\n",
      " [ 0.07139652]\n",
      " [ 0.01427248]\n",
      " [-0.00836158]\n",
      " [-0.06764124]\n",
      " [-0.0105172 ]\n",
      " [-0.02345095]\n",
      " [ 0.06816308]\n",
      " [-0.03530688]\n",
      " [-0.01159501]\n",
      " [-0.0730303 ]\n",
      " [-0.04177375]\n",
      " [ 0.01427248]\n",
      " [-0.00728377]\n",
      " [ 0.0164281 ]\n",
      " [-0.00943939]\n",
      " [-0.01590626]\n",
      " [ 0.0250506 ]\n",
      " [-0.04931844]\n",
      " [ 0.04121778]\n",
      " [-0.06332999]\n",
      " [-0.06440781]\n",
      " [-0.02560657]\n",
      " [-0.00405033]\n",
      " [ 0.00457217]\n",
      " [-0.00728377]\n",
      " [-0.0374625 ]\n",
      " [-0.02560657]\n",
      " [-0.02452876]\n",
      " [-0.01806189]\n",
      " [-0.01482845]\n",
      " [-0.02991782]\n",
      " [-0.046085  ]\n",
      " [-0.06979687]\n",
      " [ 0.03367309]\n",
      " [-0.00405033]\n",
      " [-0.02021751]\n",
      " [ 0.00241654]\n",
      " [-0.03099563]\n",
      " [ 0.02828403]\n",
      " [-0.03638469]\n",
      " [-0.05794093]\n",
      " [-0.0374625 ]\n",
      " [ 0.01211685]\n",
      " [-0.02237314]\n",
      " [-0.03530688]\n",
      " [ 0.00996123]\n",
      " [-0.03961813]\n",
      " [ 0.07139652]\n",
      " [-0.07518593]\n",
      " [-0.00620595]\n",
      " [-0.04069594]\n",
      " [-0.04824063]\n",
      " [-0.02560657]\n",
      " [ 0.0519959 ]\n",
      " [ 0.00457217]\n",
      " [-0.06440781]\n",
      " [-0.01698407]\n",
      " [-0.05794093]\n",
      " [ 0.00996123]\n",
      " [ 0.08864151]\n",
      " [-0.00512814]\n",
      " [-0.06440781]\n",
      " [ 0.01750591]\n",
      " [-0.04500719]\n",
      " [ 0.02828403]\n",
      " [ 0.04121778]\n",
      " [ 0.06492964]\n",
      " [-0.03207344]\n",
      " [-0.07626374]\n",
      " [ 0.04984027]\n",
      " [ 0.04552903]\n",
      " [-0.00943939]\n",
      " [-0.03207344]\n",
      " [ 0.00457217]\n",
      " [ 0.02073935]\n",
      " [ 0.01427248]\n",
      " [ 0.11019775]\n",
      " [ 0.00133873]\n",
      " [ 0.05846277]\n",
      " [-0.02129532]\n",
      " [-0.0105172 ]\n",
      " [-0.04716281]\n",
      " [ 0.00457217]\n",
      " [ 0.01750591]\n",
      " [ 0.08109682]\n",
      " [ 0.0347509 ]\n",
      " [ 0.02397278]\n",
      " [-0.00836158]\n",
      " [-0.06117437]\n",
      " [-0.00189471]\n",
      " [-0.06225218]\n",
      " [ 0.0164281 ]\n",
      " [ 0.09618619]\n",
      " [-0.06979687]\n",
      " [-0.02129532]\n",
      " [-0.05362969]\n",
      " [ 0.0433734 ]\n",
      " [ 0.05630715]\n",
      " [-0.0816528 ]\n",
      " [ 0.04984027]\n",
      " [ 0.11127556]\n",
      " [ 0.06169621]\n",
      " [ 0.01427248]\n",
      " [ 0.04768465]\n",
      " [ 0.01211685]\n",
      " [ 0.00564998]\n",
      " [ 0.04660684]\n",
      " [ 0.12852056]\n",
      " [ 0.05954058]\n",
      " [ 0.09295276]\n",
      " [ 0.01535029]\n",
      " [-0.00512814]\n",
      " [ 0.0703187 ]\n",
      " [-0.00405033]\n",
      " [-0.00081689]\n",
      " [-0.04392938]\n",
      " [ 0.02073935]\n",
      " [ 0.06061839]\n",
      " [-0.0105172 ]\n",
      " [-0.03315126]\n",
      " [-0.06548562]\n",
      " [ 0.0433734 ]\n",
      " [-0.06225218]\n",
      " [ 0.06385183]\n",
      " [ 0.03043966]\n",
      " [ 0.07247433]\n",
      " [-0.0191397 ]\n",
      " [-0.06656343]\n",
      " [-0.06009656]\n",
      " [ 0.06924089]\n",
      " [ 0.05954058]\n",
      " [-0.02668438]\n",
      " [-0.02021751]\n",
      " [-0.046085  ]\n",
      " [ 0.07139652]\n",
      " [-0.07949718]\n",
      " [ 0.00996123]\n",
      " [-0.03854032]\n",
      " [ 0.01966154]\n",
      " [ 0.02720622]\n",
      " [-0.00836158]\n",
      " [-0.01590626]\n",
      " [ 0.00457217]\n",
      " [-0.04285156]\n",
      " [ 0.00564998]\n",
      " [-0.03530688]\n",
      " [ 0.02397278]\n",
      " [-0.01806189]\n",
      " [ 0.04229559]\n",
      " [-0.0547075 ]\n",
      " [-0.00297252]\n",
      " [-0.06656343]\n",
      " [-0.01267283]\n",
      " [-0.04177375]\n",
      " [-0.03099563]\n",
      " [-0.00512814]\n",
      " [-0.05901875]\n",
      " [ 0.0250506 ]\n",
      " [-0.046085  ]\n",
      " [ 0.00349435]\n",
      " [ 0.05415152]\n",
      " [-0.04500719]\n",
      " [-0.05794093]\n",
      " [-0.05578531]\n",
      " [ 0.00133873]\n",
      " [ 0.03043966]\n",
      " [ 0.00672779]\n",
      " [ 0.04660684]\n",
      " [ 0.02612841]\n",
      " [ 0.04552903]\n",
      " [ 0.04013997]\n",
      " [-0.01806189]\n",
      " [ 0.01427248]\n",
      " [ 0.03690653]\n",
      " [ 0.00349435]\n",
      " [-0.07087468]\n",
      " [-0.03315126]\n",
      " [ 0.09403057]\n",
      " [ 0.03582872]\n",
      " [ 0.03151747]\n",
      " [-0.06548562]\n",
      " [-0.04177375]\n",
      " [-0.03961813]\n",
      " [-0.03854032]\n",
      " [-0.02560657]\n",
      " [-0.02345095]\n",
      " [-0.06656343]\n",
      " [ 0.03259528]\n",
      " [-0.046085  ]\n",
      " [-0.02991782]\n",
      " [-0.01267283]\n",
      " [-0.01590626]\n",
      " [ 0.07139652]\n",
      " [-0.03099563]\n",
      " [ 0.00026092]\n",
      " [ 0.03690653]\n",
      " [ 0.03906215]\n",
      " [-0.01482845]\n",
      " [ 0.00672779]\n",
      " [-0.06871905]\n",
      " [-0.00943939]\n",
      " [ 0.01966154]\n",
      " [ 0.07462995]\n",
      " [-0.00836158]\n",
      " [-0.02345095]\n",
      " [-0.046085  ]\n",
      " [ 0.05415152]\n",
      " [-0.03530688]\n",
      " [-0.03207344]\n",
      " [-0.0816528 ]\n",
      " [ 0.04768465]\n",
      " [ 0.06061839]\n",
      " [ 0.05630715]\n",
      " [ 0.09834182]\n",
      " [ 0.05954058]\n",
      " [ 0.03367309]\n",
      " [ 0.05630715]\n",
      " [-0.06548562]\n",
      " [ 0.16085492]\n",
      " [-0.05578531]\n",
      " [-0.02452876]\n",
      " [-0.03638469]\n",
      " [-0.00836158]\n",
      " [-0.04177375]\n",
      " [ 0.12744274]\n",
      " [-0.07734155]\n",
      " [ 0.02828403]\n",
      " [-0.02560657]\n",
      " [-0.06225218]\n",
      " [-0.00081689]\n",
      " [ 0.08864151]\n",
      " [-0.03207344]\n",
      " [ 0.03043966]\n",
      " [ 0.00888341]\n",
      " [ 0.00672779]\n",
      " [-0.02021751]\n",
      " [-0.02452876]\n",
      " [-0.01159501]\n",
      " [ 0.02612841]\n",
      " [-0.05901875]\n",
      " [-0.03638469]\n",
      " [-0.02452876]\n",
      " [ 0.01858372]\n",
      " [-0.0902753 ]\n",
      " [-0.00512814]\n",
      " [-0.05255187]\n",
      " [-0.02237314]\n",
      " [-0.02021751]\n",
      " [-0.0547075 ]\n",
      " [-0.00620595]\n",
      " [-0.01698407]\n",
      " [ 0.05522933]\n",
      " [ 0.07678558]\n",
      " [ 0.01858372]\n",
      " [-0.02237314]\n",
      " [ 0.09295276]\n",
      " [-0.03099563]\n",
      " [ 0.03906215]\n",
      " [-0.06117437]\n",
      " [-0.00836158]\n",
      " [-0.0374625 ]\n",
      " [-0.01375064]\n",
      " [ 0.07355214]\n",
      " [-0.02452876]\n",
      " [ 0.03367309]\n",
      " [ 0.0347509 ]\n",
      " [-0.03854032]\n",
      " [-0.03961813]\n",
      " [-0.00189471]\n",
      " [-0.03099563]\n",
      " [-0.046085  ]\n",
      " [ 0.00133873]\n",
      " [ 0.06492964]\n",
      " [ 0.04013997]\n",
      " [-0.02345095]\n",
      " [ 0.05307371]\n",
      " [ 0.04013997]\n",
      " [-0.02021751]\n",
      " [ 0.01427248]\n",
      " [-0.03422907]\n",
      " [ 0.00672779]\n",
      " [ 0.00457217]\n",
      " [ 0.03043966]\n",
      " [ 0.0519959 ]\n",
      " [ 0.06169621]\n",
      " [-0.00728377]\n",
      " [ 0.00564998]\n",
      " [ 0.05415152]\n",
      " [-0.00836158]\n",
      " [ 0.114509  ]\n",
      " [ 0.06708527]\n",
      " [-0.05578531]\n",
      " [ 0.03043966]\n",
      " [-0.02560657]\n",
      " [ 0.10480869]\n",
      " [-0.00620595]\n",
      " [-0.04716281]\n",
      " [-0.04824063]\n",
      " [ 0.08540807]\n",
      " [-0.01267283]\n",
      " [-0.03315126]\n",
      " [-0.00728377]\n",
      " [-0.01375064]\n",
      " [ 0.05954058]\n",
      " [ 0.02181716]\n",
      " [ 0.01858372]\n",
      " [-0.01159501]\n",
      " [-0.00297252]\n",
      " [ 0.01750591]\n",
      " [-0.02991782]\n",
      " [-0.02021751]\n",
      " [-0.05794093]\n",
      " [ 0.06061839]\n",
      " [-0.04069594]\n",
      " [-0.07195249]\n",
      " [-0.05578531]\n",
      " [ 0.04552903]\n",
      " [-0.00943939]\n",
      " [-0.03315126]\n",
      " [ 0.04984027]\n",
      " [-0.08488624]\n",
      " [ 0.00564998]\n",
      " [ 0.02073935]\n",
      " [-0.00728377]\n",
      " [ 0.10480869]\n",
      " [-0.02452876]\n",
      " [-0.00620595]\n",
      " [-0.03854032]\n",
      " [ 0.13714305]\n",
      " [ 0.17055523]\n",
      " [ 0.00241654]\n",
      " [ 0.03798434]\n",
      " [-0.05794093]\n",
      " [-0.00943939]\n",
      " [-0.02345095]\n",
      " [-0.0105172 ]\n",
      " [-0.03422907]\n",
      " [-0.00297252]\n",
      " [ 0.06816308]\n",
      " [ 0.00996123]\n",
      " [ 0.00241654]\n",
      " [-0.03854032]\n",
      " [ 0.02612841]\n",
      " [-0.08919748]\n",
      " [ 0.06061839]\n",
      " [-0.02884001]\n",
      " [-0.02991782]\n",
      " [-0.0191397 ]\n",
      " [-0.04069594]\n",
      " [ 0.01535029]\n",
      " [-0.02452876]\n",
      " [ 0.00133873]\n",
      " [ 0.06924089]\n",
      " [-0.06979687]\n",
      " [-0.02991782]\n",
      " [-0.046085  ]\n",
      " [ 0.01858372]\n",
      " [ 0.00133873]\n",
      " [-0.03099563]\n",
      " [-0.00405033]\n",
      " [ 0.01535029]\n",
      " [ 0.02289497]\n",
      " [ 0.04552903]\n",
      " [-0.04500719]\n",
      " [-0.03315126]\n",
      " [ 0.097264  ]\n",
      " [ 0.05415152]\n",
      " [ 0.12313149]\n",
      " [-0.08057499]\n",
      " [ 0.09295276]\n",
      " [-0.05039625]\n",
      " [-0.01159501]\n",
      " [-0.0277622 ]\n",
      " [ 0.05846277]\n",
      " [ 0.08540807]\n",
      " [-0.00081689]\n",
      " [ 0.00672779]\n",
      " [ 0.00888341]\n",
      " [ 0.08001901]\n",
      " [ 0.07139652]\n",
      " [-0.02452876]\n",
      " [-0.0547075 ]\n",
      " [-0.03638469]\n",
      " [ 0.0164281 ]\n",
      " [ 0.07786339]\n",
      " [-0.03961813]\n",
      " [ 0.01103904]\n",
      " [-0.04069594]\n",
      " [-0.03422907]\n",
      " [ 0.00564998]\n",
      " [ 0.08864151]\n",
      " [-0.03315126]\n",
      " [-0.05686312]\n",
      " [-0.03099563]\n",
      " [ 0.05522933]\n",
      " [-0.06009656]\n",
      " [ 0.00133873]\n",
      " [-0.02345095]\n",
      " [-0.07410811]\n",
      " [ 0.01966154]\n",
      " [-0.01590626]\n",
      " [-0.01590626]\n",
      " [ 0.03906215]\n",
      " [-0.0730303 ]]\n",
      "[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n",
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990749\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06833155\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286131\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04688253\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452873\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00422151\n",
      "   0.00306441]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y = True)\n",
    "\n",
    "diabetes_X_new = diabetes_X[:,np.newaxis, 2] #bmi\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes_X_new, diabetes_y, test_size = 0.2)\n",
    "\n",
    "degree = 1000 #차수\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(degree), linear_model.LinearRegression())\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_plot= np.linspace(X_test.min(),X_test.max(),100).reshape(-1, 1)\n",
    "y_pred = model.predict(X_plot)\n",
    "\n",
    "\n",
    "\n",
    "# print(diabetes_X_new)\n",
    "# print(diabetes_y)\n",
    "# print(diabetes_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26d9f39e-1105-47c3-8863-0239396d226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#미분치\n",
    "def sigmoid_deriv(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "learning_rate = 0.2\n",
    "\n",
    "input = [[0,0],[0,1],[1,0],[1,1]]\n",
    "\n",
    "targets = [1, 0, 0, 1]\n",
    "\n",
    "#은닉층 1\n",
    "w1_11 = 0.10\n",
    "w1_21 = 0.30\n",
    "b1_1 = 0.1\n",
    "# 은닉층 2\n",
    "w1_12 = 0.10\n",
    "w1_22 = 0.30\n",
    "b1_2 = 0.1\n",
    "\n",
    "# 은닉층 2\n",
    "w1_13 = 0.10\n",
    "w1_23 = 0.30\n",
    "b1_3 = 0.1\n",
    "\n",
    "#출력층\n",
    "w2_11 = 0.50\n",
    "w2_21 = 0.60\n",
    "w2_31 = 0.30\n",
    "b2 = 0.3\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    global w1_11 ,w1_21 ,b1_1 , w1_12 , w1_22, b1_2, w1_13 , w1_23, b1_3, w2_11 ,w2_21 ,w2_31 ,b2\n",
    "\n",
    "    x1 , x2 =x\n",
    "    z1 = x1 * w1_11+ x2 * w1_21 + b1_1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    z2 = x1 * w1_12+ x2 * w1_22 + b1_2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    z3 = x1 * w1_13+ x2 * w1_23 + b1_3\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    z_out = a1 * w2_11 + a2 * w2_21+ a3 * w2_31 + b2\n",
    "    a_out =sigmoid(z_out)\n",
    "\n",
    "    return a1, a2 , a3, a_out\n",
    "\n",
    "def backward(x, target ,a1, a2 , a3, a_out):\n",
    "    global w1_11 ,w1_21 ,b1_1 , w1_12 , w1_22, b1_2, w1_13 , w1_23, b1_3, w2_11 ,w2_21 ,w2_31 ,b2\n",
    "\n",
    "    x1, x2 = x\n",
    "    error = a_out - target\n",
    "    delta_out = error * sigmoid_deriv(a_out)\n",
    "\n",
    "    delta1 = delta_out * w2_11 * sigmoid_deriv(a1)\n",
    "    delta2 = delta_out * w2_21 * sigmoid_deriv(a2)\n",
    "    delta3 = delta_out * w2_31 * sigmoid_deriv(a3)\n",
    "\n",
    "    w2_11 -= learning_rate * a1 * delta_out\n",
    "    w2_21 -= learning_rate * a2 * delta_out\n",
    "    w2_31 -= learning_rate * a3 * delta_out\n",
    "    b2 -= learning_rate * delta_out\n",
    "\n",
    "    w1_11 -= learning_rate * x1 * delta1\n",
    "    w1_21 -= learning_rate * x2 * delta1\n",
    "    b1_1 -= learning_rate *delta1\n",
    "    \n",
    "    w1_12 -= learning_rate * x1 * delta1\n",
    "    w1_22 -= learning_rate * x2 * delta1\n",
    "    b1_2 -= learning_rate *delta1\n",
    "\n",
    "    w1_13 -= learning_rate * x1 * delta1\n",
    "    w1_23 -= learning_rate * x2 * delta1\n",
    "    b1_3 -= learning_rate *delta1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684acb9e-9f84-4eae-a0a7-c3444a12af17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
